filter {
    #if this is a field we aggregate, then do it
    if [@do_aggregation] {
        #calculate aggregation id
        fingerprint {
        source => [ '[meta][id]', '[start]']
        concatenate_sources => true
        method              => 'SHA256'
        target              => 'bucket_id'
        key                 => 'snmp bucket id'
        }

        #now aggregate
        aggregate {
            # use this to organize what gets aggregated
            task_id => "%{[bucket_id]}"
            # make sure the bucket id is in event
            timeout_task_id_field => "[bucket_id]"
            # create a new event after everything as been aggregated
            push_map_as_event_on_timeout => true
            # timeout after seconds indicated below
            timeout => "${logstash_agg_timeout:30}"
            # timeout based on polling time. Since this is best effort 
            # will prevent backlog from filling logstash at the expense
            # of more events per second
            timeout_timestamp_field => "start"
            # save the aggregation maps here in case logstash dies 
            ##  (use a different file for each logstash pipeline!)
            aggregate_maps_path => '/tmp/logstash-snmp-agg-maps'
            # Code to perform aggregation
            code => "
                #subroutine for merging values
                def deep_hash_merge(h1, h2)
                    Array(h2).each do |k, v|
                        if v.is_a?(Hash) && h1.key?(k) && h1[k].is_a?(Hash) then
                            #if we have an existing hash, then merge...
                            deep_hash_merge(h1[k], v)
                        else
                            #...otherwise write/overwrite
                            h1[k] = v
                        end
                    end
                end

                # keep track of how many events we aggregate
                map['agg_events'] ||= 0
                map['agg_events'] += 1

                # Copy over fields from first event
                #   ||= means set if not already set
                map['start'] ||= event.get('start')
                map['type'] ||= event.get('type')
                map['@ingest_time_min'] ||= event.get('@ingest_time')
                map['@ingest_time_max'] ||= event.get('@ingest_time')
                map['@collect_time_min'] ||= event.get('@collect_time')
                map['@collect_time_max'] ||= event.get('@collect_time')
                map['@kafka'] ||= event.get('@kafka')
                map['@kafka_agg'] ||= event.get('@kafka_agg')
                map['meta'] ||= event.get('meta')
                map['values'] ||= event.get('values')
                map['@metadata'] ||= {}
                map['@metadata']['sensor_map'] ||= {}
                #always set sensor - could be multiple
                sensor = event.get('[meta][sensor_id]')
                if sensor then
                    map['@metadata']['sensor_map'][sensor] = true
                    #delete the singular sensor_id
                    map['meta'].delete('sensor_id')
                end

                #Aggregate fields from second event
                if map['agg_events'] > 1 then
                    #calc min and max ingest time
                    ingest_time = event.get('@ingest_time')
                    if ingest_time && ingest_time < map['@ingest_time_min'] then
                        map['@ingest_time_min'] = ingest_time
                    end
                    if ingest_time && ingest_time > map['@ingest_time_max'] then
                        map['@ingest_time_max'] = ingest_time
                    end

                    #calc min and max collect time
                    collect_time = event.get('@collect_time')
                    if collect_time && collect_time < map['@collect_time_min'] then
                        map['@collect_time_min'] = collect_time
                    end
                    if collect_time && collect_time > map['@collect_time_max'] then
                        map['@collect_time_max'] = collect_time
                    end

                    # Deep merge values
                    # have checked values multiple times so should not be nil
                    deep_hash_merge(map['values'], event.get('values'))
                end

                #cancel original event
                event.cancel()
            "
            #once aggregated event has fired do some final tasks
            timeout_code => "
                #merge sensor ids - convert hash keys to list
                sensor_map = event.get('[@metadata][sensor_map]')
                if sensor_map && sensor_map.kind_of?(Hash) then
                    event.set('[meta][sensor_id]', sensor_map.keys)
                end
            "
        }
    } else {
        #clean out fields normally cleaned out by aggregation
        prune {
            whitelist_names => ["start", "type", "@ingest_time", "@collect_time", "@kafka", "@kafka_agg", "meta", "values", "@metadata"]
        }
    }
}